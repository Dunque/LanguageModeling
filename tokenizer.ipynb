{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to load the Tokenizer class in Keras"
      ],
      "metadata": {
        "id": "xsEoUNRzSSdB"
      },
      "id": "xsEoUNRzSSdB"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c1b0222a-e05d-4901-a503-94920d9d296a",
      "metadata": {
        "id": "c1b0222a-e05d-4901-a503-94920d9d296a"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.client import device_lib\n",
        "import tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Current keras version: \", tensorflow.keras.__version__)\n",
        "print (\"Available resources: \", device_lib.list_local_devices())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUmy0FcTRJP2",
        "outputId": "79c71455-8272-4dca-cbf1-5eb38b90677b"
      },
      "id": "YUmy0FcTRJP2",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current keras version:  2.8.0\n",
            "Available resources:  [name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 4088150603172129951\n",
            "xla_global_id: -1\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to transform texts to IDs. IDs are assigned based on frequency, i.e., the most common words will receive ID #1**"
      ],
      "metadata": {
        "id": "T4FrLhohpaFD"
      },
      "id": "T4FrLhohpaFD"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "texts = [\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do.\", \n",
        "         \"So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid).\",\n",
        "         \"There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself.\"]\n",
        "tokenizer.fit_on_texts(texts) #It trains a tokenizer given a list of texts\n",
        "\n",
        "texts2ids = tokenizer.texts_to_sequences(texts) #Tokenizes the texts, and transform them into IDs\n",
        "print (\"Texts as IDs:\", texts2ids)\n",
        "ids2texts = tokenizer.sequences_to_texts(texts2ids)\n",
        "print (\"IDs back to texts:\", ids2texts)\n",
        "\n"
      ],
      "metadata": {
        "id": "aKeBJLbSUbmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4d8865-a2ee-4f96-d276-4c8f53975c43"
      },
      "id": "aKeBJLbSUbmr",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts as IDs: [[8, 4, 14, 1, 15, 2, 16, 5, 17, 18, 6, 19, 20, 3, 21, 9, 5, 22, 10, 1, 23], [7, 11, 4, 24, 12, 6, 25, 26, 13, 27, 13, 11, 28, 29, 3, 30, 31, 32, 6, 33, 2, 34, 9, 35], [36, 4, 10, 7, 2, 37, 12, 38, 39, 40, 8, 41, 42, 7, 2, 43, 44, 5, 3, 45, 1, 46, 3, 47, 48, 1, 49]]\n",
            "IDs back to texts: ['alice was beginning to get very tired of sitting by her sister on the bank and of having nothing to do', 'so she was considering in her own mind as well as she could for the hot day made her feel very sleepy and stupid', 'there was nothing so very remarkable in that nor did alice think it so very much out of the way to hear the rabbit say to itself']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can consult what was been learned by the Tokenizer, e.g., to obtain the index of a given word using tokenizer.word_index"
      ],
      "metadata": {
        "id": "EJ12IRDs8Yg3"
      },
      "id": "EJ12IRDs8Yg3"
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"tokenizer.word_counts:\", tokenizer.word_counts)\n",
        "print (\"tokenizer.document_counts:\", tokenizer.document_count)\n",
        "print (\"tokenizer.word_index:\",tokenizer.word_index)\n",
        "print (\"tokenizer.word_docs\", tokenizer.word_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8_Qfy-Z7ujY",
        "outputId": "c6a0a682-5e74-413e-e9da-9b1ecede72e9"
      },
      "id": "x8_Qfy-Z7ujY",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer.word_counts: OrderedDict([('alice', 2), ('was', 3), ('beginning', 1), ('to', 4), ('get', 1), ('very', 4), ('tired', 1), ('of', 3), ('sitting', 1), ('by', 1), ('her', 3), ('sister', 1), ('on', 1), ('the', 4), ('bank', 1), ('and', 2), ('having', 1), ('nothing', 2), ('do', 1), ('so', 3), ('she', 2), ('considering', 1), ('in', 2), ('own', 1), ('mind', 1), ('as', 2), ('well', 1), ('could', 1), ('for', 1), ('hot', 1), ('day', 1), ('made', 1), ('feel', 1), ('sleepy', 1), ('stupid', 1), ('there', 1), ('remarkable', 1), ('that', 1), ('nor', 1), ('did', 1), ('think', 1), ('it', 1), ('much', 1), ('out', 1), ('way', 1), ('hear', 1), ('rabbit', 1), ('say', 1), ('itself', 1)])\n",
            "tokenizer.document_counts: 3\n",
            "tokenizer.word_index: {'to': 1, 'very': 2, 'the': 3, 'was': 4, 'of': 5, 'her': 6, 'so': 7, 'alice': 8, 'and': 9, 'nothing': 10, 'she': 11, 'in': 12, 'as': 13, 'beginning': 14, 'get': 15, 'tired': 16, 'sitting': 17, 'by': 18, 'sister': 19, 'on': 20, 'bank': 21, 'having': 22, 'do': 23, 'considering': 24, 'own': 25, 'mind': 26, 'well': 27, 'could': 28, 'for': 29, 'hot': 30, 'day': 31, 'made': 32, 'feel': 33, 'sleepy': 34, 'stupid': 35, 'there': 36, 'remarkable': 37, 'that': 38, 'nor': 39, 'did': 40, 'think': 41, 'it': 42, 'much': 43, 'out': 44, 'way': 45, 'hear': 46, 'rabbit': 47, 'say': 48, 'itself': 49}\n",
            "tokenizer.word_docs defaultdict(<class 'int'>, {'the': 3, 'sitting': 1, 'by': 1, 'alice': 2, 'to': 2, 'bank': 1, 'and': 2, 'nothing': 2, 'on': 1, 'tired': 1, 'do': 1, 'sister': 1, 'was': 3, 'having': 1, 'her': 2, 'get': 1, 'of': 2, 'very': 3, 'beginning': 1, 'she': 1, 'in': 2, 'mind': 1, 'made': 1, 'as': 1, 'own': 1, 'considering': 1, 'feel': 1, 'could': 1, 'day': 1, 'sleepy': 1, 'so': 2, 'stupid': 1, 'well': 1, 'hot': 1, 'for': 1, 'much': 1, 'think': 1, 'remarkable': 1, 'out': 1, 'say': 1, 'that': 1, 'there': 1, 'itself': 1, 'nor': 1, 'it': 1, 'rabbit': 1, 'did': 1, 'hear': 1, 'way': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that punctuation has been removed, the text has been lowercased, etc. This is becuase the default attribute values of the Tokenizer class. They can be consulted at: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
        "\n",
        "    tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=None,\n",
        "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "        lower=True, split=' ', char_level=False, oov_token=None,\n",
        "        document_count=0, **kwargs\n",
        "    )\n",
        "\n",
        "You have the freedom to set up the other parameters to you favorite choice, and use them in the assigments as you please to create your LM.\n",
        "\n",
        "More particularly, in the lab assignment we will modify two parameters: num_words and oov_token:\n",
        "\n",
        "**num_words**: the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
        "\n",
        "**oov_token**: if given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TKi5cbWGuF7S"
      },
      "id": "TKi5cbWGuF7S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to save and load a tokenizer**"
      ],
      "metadata": {
        "id": "j59_P84I4GzX"
      },
      "id": "j59_P84I4GzX"
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle \n",
        "\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "          pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "          tokenizer = pickle.load(handle)\n",
        "  "
      ],
      "metadata": {
        "id": "DnUb6iNk3s76"
      },
      "id": "DnUb6iNk3s76",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example of a Tokenizer considering oov_tokens**"
      ],
      "metadata": {
        "id": "RJNGaIGSHc9P"
      },
      "id": "RJNGaIGSHc9P"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(oov_token=\"<unk>\")\n",
        "texts = [\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do.\", \n",
        "         \"So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid).\",\n",
        "         \"There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself.\"]\n",
        "tokenizer.fit_on_texts(texts) \n",
        "\n",
        "oov_texts = [\"Harry Potter and the Philoshopher stone\"]\n",
        "texts2ids = tokenizer.texts_to_sequences(oov_texts) #\n",
        "print (\"Texts as IDs:\", texts2ids)\n",
        "ids2texts = tokenizer.sequences_to_texts(texts2ids)\n",
        "print (\"IDs back to texts:\", ids2texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOr_dNOyGuSQ",
        "outputId": "6f8cdd05-52c3-4b3b-9a96-d1bac2ad9cec"
      },
      "id": "HOr_dNOyGuSQ",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts as IDs: [[1, 1, 10, 4, 1, 1]]\n",
            "IDs back to texts: ['<unk> <unk> and the <unk> <unk>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example of a Tokenizer considering oov_tokens and limited num_words**"
      ],
      "metadata": {
        "id": "IT_kZLN2HifI"
      },
      "id": "IT_kZLN2HifI"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(oov_token=\"<unk>\", num_words=15)\n",
        "texts = [\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do.\", \n",
        "         \"So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid).\",\n",
        "         \"There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself.\"]\n",
        "tokenizer.fit_on_texts(texts) \n",
        "\n",
        "texts2ids = tokenizer.texts_to_sequences(texts) #\n",
        "print (\"Texts as IDs:\", texts2ids)\n",
        "ids2texts = tokenizer.sequences_to_texts(texts2ids)\n",
        "print (\"IDs back to texts:\", ids2texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkwf2rBfHnrf",
        "outputId": "2c45ba92-ddbe-45ae-faf6-e6895ffafc46"
      },
      "id": "qkwf2rBfHnrf",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts as IDs: [[9, 5, 1, 2, 1, 3, 1, 6, 1, 1, 7, 1, 1, 4, 1, 10, 6, 1, 11, 2, 1], [8, 12, 5, 1, 13, 7, 1, 1, 14, 1, 14, 12, 1, 1, 4, 1, 1, 1, 7, 1, 3, 1, 10, 1], [1, 5, 11, 8, 3, 1, 13, 1, 1, 1, 9, 1, 1, 8, 3, 1, 1, 6, 4, 1, 2, 1, 4, 1, 1, 2, 1]]\n",
            "IDs back to texts: ['alice was <unk> to <unk> very <unk> of <unk> <unk> her <unk> <unk> the <unk> and of <unk> nothing to <unk>', 'so she was <unk> in her <unk> <unk> as <unk> as she <unk> <unk> the <unk> <unk> <unk> her <unk> very <unk> and <unk>', '<unk> was nothing so very <unk> in <unk> <unk> <unk> alice <unk> <unk> so very <unk> <unk> of the <unk> to <unk> the <unk> <unk> to <unk>']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "tokenizer.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}